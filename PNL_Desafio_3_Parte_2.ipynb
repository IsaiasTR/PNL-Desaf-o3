{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de lenguaje con tokenización por caracteres"
      ],
      "metadata": {
        "id": "xej9goST8om6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ],
      "metadata": {
        "id": "GRRBKixw8waA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importamos las librerias necesarias**"
      ],
      "metadata": {
        "id": "s5rYFGVA9ZwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "K6pEY-Cl-IqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization"
      ],
      "metadata": {
        "id": "7vkvbCHX8vkG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cargamos el conjunto de datos**"
      ],
      "metadata": {
        "id": "rcPEhjiL9jVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['sci.space', 'comp.graphics', 'rec.sport.hockey']\n",
        "newsgroups_data = fetch_20newsgroups(subset='all', categories=categories)\n",
        "corpus = newsgroups_data.data"
      ],
      "metadata": {
        "id": "x0OWzgzg9kCI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenización**"
      ],
      "metadata": {
        "id": "7Gvi-cYo-Yos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)"
      ],
      "metadata": {
        "id": "Idq9GnIx-ccW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding para asegurar que todas las secuencias tengan el mismo largo**"
      ],
      "metadata": {
        "id": "jM5BqgRq-fyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 100\n",
        "data = pad_sequences(sequences, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "SQdNrULM-lCA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dividimos en entrenamiento y validación**"
      ],
      "metadata": {
        "id": "0d4Hu3l7-qVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "5lwg8KFx-u5t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estructuración del dataset**"
      ],
      "metadata": {
        "id": "SyA955lF-0vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sequences = X_train[:, :-1]\n",
        "y_train = X_train[:, -1]\n",
        "\n",
        "X_val_sequences = X_val[:, :-1]\n",
        "y_val = X_val[:, -1]"
      ],
      "metadata": {
        "id": "MeD5ZuMT-6el"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arquitectura**"
      ],
      "metadata": {
        "id": "evR6bYmW_CBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el modelo\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen-1))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMOjzBA6_FHd",
        "outputId": "a61430bd-3c92-4616-de38-e6e92dcaa933"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento del modelo**"
      ],
      "metadata": {
        "id": "pWAsDWJ1_T46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_sequences, y_train, epochs=10, validation_data=(X_val_sequences, y_val), batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VjU4vH2_XlZ",
        "outputId": "a92e053b-3ffa-4cd3-ad6f-553e86f728fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 400ms/step - accuracy: 0.0606 - loss: 10.4403 - val_accuracy: 0.0794 - val_loss: 9.0152\n",
            "Epoch 2/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 425ms/step - accuracy: 0.0868 - loss: 8.4972 - val_accuracy: 0.0794 - val_loss: 8.1666\n",
            "Epoch 3/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 406ms/step - accuracy: 0.0816 - loss: 7.7069 - val_accuracy: 0.0794 - val_loss: 7.8904\n",
            "Epoch 4/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 385ms/step - accuracy: 0.0851 - loss: 7.2775 - val_accuracy: 0.0794 - val_loss: 7.7740\n",
            "Epoch 5/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 390ms/step - accuracy: 0.0778 - loss: 7.0253 - val_accuracy: 0.0794 - val_loss: 7.7169\n",
            "Epoch 6/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 393ms/step - accuracy: 0.0828 - loss: 6.8333 - val_accuracy: 0.0794 - val_loss: 7.6889\n",
            "Epoch 7/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 425ms/step - accuracy: 0.0853 - loss: 6.7728 - val_accuracy: 0.0794 - val_loss: 7.6677\n",
            "Epoch 8/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 379ms/step - accuracy: 0.0867 - loss: 6.7014 - val_accuracy: 0.0794 - val_loss: 7.6556\n",
            "Epoch 9/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 390ms/step - accuracy: 0.0830 - loss: 6.6705 - val_accuracy: 0.0794 - val_loss: 7.6521\n",
            "Epoch 10/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 398ms/step - accuracy: 0.0813 - loss: 6.6376 - val_accuracy: 0.0794 - val_loss: 7.6454\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79b445add540>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentario**\n",
        "\n",
        "El modelo de lenguaje entrenado mostró una pérdida en descenso (de 10.44 a 6.63) a lo largo de 10 épocas, pero la precisión se mantuvo baja (aproximadamente 8%) tanto en el conjunto de entrenamiento como en el de validación. Esto sugiere que el modelo no logró aprender de manera efectiva los patrones contextuales en el corpus. Es probable que el modelo sea demasiado simple o que se requieran ajustes en los hiperparámetros y el preprocesamiento."
      ],
      "metadata": {
        "id": "dnRzhWITCSdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mejoras implementadas**\n",
        "\n",
        "* Más capas LSTM: Ahora tienes una red más profunda, con dos capas LSTM.\n",
        "* Dropout: Se añadió Dropout para prevenir sobreajuste.\n",
        "* Batch Normalization: Estabiliza y acelera el proceso de aprendizaje.\n",
        "* Mayor dimensión de embeddings: Se aumentó a 256 para capturar mejor las relaciones entre palabras.\n",
        "*Optimizador Adam: Mejora la velocidad de convergencia y rendimiento."
      ],
      "metadata": {
        "id": "-yA-sIViDSOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos los parámetros\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 256  # Aumentamos la dimensión del embedding\n",
        "maxlen = 100\n",
        "\n",
        "# Creamos el modelo\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen-1))\n",
        "\n",
        "# Añadimos una capa LSTM adicional con return_sequences=True\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.2))  # Añadimos Dropout para evitar el sobreajuste\n",
        "\n",
        "# Segunda capa LSTM para profundizar el modelo\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Añadimos Batch Normalization para estabilizar el entrenamiento\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Capa de salida con activación softmax\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Compilamos el modelo con el optimizador Adam\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamos el modelo\n",
        "model.fit(X_train_sequences, y_train, epochs=10, validation_data=(X_val_sequences, y_val), batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V80ulEkADuUv",
        "outputId": "346e470d-c4c3-4922-e994-90fd0c6c8c80"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 816ms/step - accuracy: 0.0788 - loss: 10.5063 - val_accuracy: 0.1639 - val_loss: 10.4400\n",
            "Epoch 2/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 799ms/step - accuracy: 0.1744 - loss: 9.3215 - val_accuracy: 0.1892 - val_loss: 9.8730\n",
            "Epoch 3/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 777ms/step - accuracy: 0.2437 - loss: 7.1632 - val_accuracy: 0.2044 - val_loss: 8.8916\n",
            "Epoch 4/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 785ms/step - accuracy: 0.2740 - loss: 5.2205 - val_accuracy: 0.2399 - val_loss: 7.7500\n",
            "Epoch 5/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 829ms/step - accuracy: 0.3571 - loss: 3.9533 - val_accuracy: 0.2703 - val_loss: 7.1390\n",
            "Epoch 6/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 792ms/step - accuracy: 0.4305 - loss: 3.3637 - val_accuracy: 0.2821 - val_loss: 6.8186\n",
            "Epoch 7/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 774ms/step - accuracy: 0.5068 - loss: 2.8087 - val_accuracy: 0.3041 - val_loss: 6.7057\n",
            "Epoch 8/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 778ms/step - accuracy: 0.5774 - loss: 2.3808 - val_accuracy: 0.3159 - val_loss: 6.7673\n",
            "Epoch 9/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 781ms/step - accuracy: 0.6295 - loss: 2.0768 - val_accuracy: 0.3159 - val_loss: 6.8846\n",
            "Epoch 10/10\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 896ms/step - accuracy: 0.6994 - loss: 1.6982 - val_accuracy: 0.3429 - val_loss: 7.0747\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79b445d23640>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comentario**\n",
        "\n",
        "El modelo mejoró notablemente en comparación con el modelo anterior. La precisión en el entrenamiento aumentó de 8% a 69.9% y en la validación de 7.9% a 34.3%. La pérdida de entrenamiento se redujo de 10.44 a 1.70, indicando un mejor ajuste del modelo. Aunque la pérdida de validación sigue alta (7.07), muestra una tendencia a mejorar. En general, las modificaciones realizadas han tenido un impacto positivo en el rendimiento."
      ],
      "metadata": {
        "id": "rmNZ_rt4Fe0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generación de secuencias**"
      ],
      "metadata": {
        "id": "pJ1Hft-U_f1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Greedy Search:**"
      ],
      "metadata": {
        "id": "9MMDvh72_raU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(seed_text, model, tokenizer, max_len):\n",
        "    for _ in range(max_len):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=maxlen-1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "        output_word = tokenizer.index_word[predicted[0]]\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "TIXyByYa_xPT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**probamos**"
      ],
      "metadata": {
        "id": "Bn3lnR0iGgGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"El clima en\"\n",
        "max_len = 20\n",
        "generated_text = greedy_search(seed_text, model, tokenizer, max_len)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjduIuZtGjo7",
        "outputId": "da677b14-347e-49d6-b8f8-d49246b6dc6a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "El clima en mark mark mark mark 01wb mail mail mail com edu edu mail edu edu edu au au champions champions champions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Efecto de la Temperatura en Stochastic Beam Search**"
      ],
      "metadata": {
        "id": "2CfMVYL7_yoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_with_temperature(seed_text, model, tokenizer, max_len, temperature=1.0):\n",
        "    for _ in range(max_len):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=maxlen-1, padding='pre')\n",
        "        predictions = model.predict(token_list, verbose=0)[0]\n",
        "        next_index = sample(predictions, temperature)\n",
        "        next_word = tokenizer.index_word[next_index]\n",
        "        seed_text += \" \" + next_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "rXqyJTJRAF3n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Probamos**"
      ],
      "metadata": {
        "id": "-CJ63vjVHJRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"La inteligencia artificial\"\n",
        "max_len = 20\n",
        "temperature = 0.7"
      ],
      "metadata": {
        "id": "8U7KTKKiHM-3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_with_temperature(seed_text, model, tokenizer, max_len, temperature)\n",
        "print(\"Texto Generado:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e561W6ypHYzQ",
        "outputId": "1cd48905-f2a2-420d-bb6e-69921fceb1f5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto Generado:\n",
            "La inteligencia artificial mark mark mark obligations mail 01wb 6507 01wb mark daemon thanks thanks ca edu edu 1980 edu edu canada edu\n"
          ]
        }
      ]
    }
  ]
}