{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo de lenguaje con tokenización por palabras"
      ],
      "metadata": {
        "id": "4Hwfxr_y0C5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ],
      "metadata": {
        "id": "ZRul4j16mbex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn tensorflow keras"
      ],
      "metadata": {
        "id": "HM2hrPFBtvEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importamos las librerias necesarias**"
      ],
      "metadata": {
        "id": "UrEKWjKDE3Ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Embedding\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback"
      ],
      "metadata": {
        "id": "cApqg1jqmdlT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Descargamos el dataset 20 News Groups**"
      ],
      "metadata": {
        "id": "WH8o2Tn_micP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups = fetch_20newsgroups(subset='train', categories=None)\n",
        "texts = newsgroups.data"
      ],
      "metadata": {
        "id": "U4mQtYgZmw5Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imprimimos las categorías cargadas**"
      ],
      "metadata": {
        "id": "UMvRqwQYGg2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(newsgroups.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G01haQVrGiTt",
        "outputId": "8ab7dc47-ead2-4f18-a7c3-a798055df4fe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procesamos los datos de texto**"
      ],
      "metadata": {
        "id": "gPcVIxiSm8Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convertimos a minúsculas\n",
        "    text = re.sub(r'\\d+', '', text)  # Eliminamos números\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Eliminamos espacios en blanco repetidos\n",
        "    return text\n",
        "\n",
        "texts = [preprocess_text(text) for text in texts]"
      ],
      "metadata": {
        "id": "pEZ_jGFAm8PT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenización**"
      ],
      "metadata": {
        "id": "15SC1Y6eo89G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=20000)  # Limitamos a las 20,000 palabras más frecuentes\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)"
      ],
      "metadata": {
        "id": "Tcg_WGaVo9Q5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Longitud máxima de secuencias**"
      ],
      "metadata": {
        "id": "JzRgQJDFjDix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 100"
      ],
      "metadata": {
        "id": "X9SrkJqcjI5N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Padding**"
      ],
      "metadata": {
        "id": "OykLOrX_pMAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding de las secuencias para que todas tengan la misma longitud\n",
        "X_padded = pad_sequences(sequences, maxlen=max_sequence_length)"
      ],
      "metadata": {
        "id": "__OtN9Gch5KH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la variable objetivo y la entrada (usaremos la última palabra de cada secuencia como objetivo)\n",
        "X_sequences = np.array([sequence[:-1] for sequence in X_padded if len(sequence) > 1])  # Todo menos la última palabra\n",
        "y_sequences = np.array([sequence[-1] for sequence in X_padded if len(sequence) > 1])  # La última palabra como objetivo"
      ],
      "metadata": {
        "id": "UXTLXgzRhCd-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dividimos el dataset en entrenamiento y validación**"
      ],
      "metadata": {
        "id": "0cZ3HQwTuzcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XY4qimmWiGPT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ahora definimos el modelo**"
      ],
      "metadata": {
        "id": "AOV8nQBXvD5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parámetros**"
      ],
      "metadata": {
        "id": "FOLdc7OAvNP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros del modelo\n",
        "embedding_dim = 50\n",
        "hidden_units = 64\n",
        "vocab_size = len(tokenizer.word_index) + 1  # El tamaño del vocabulario"
      ],
      "metadata": {
        "id": "idMp-P-_vJFT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback para calcular la perplejidad al final de cada época\n",
        "class PerplexityCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_loss = logs.get('val_loss')\n",
        "        if val_loss is not None:\n",
        "            perplexity = np.exp(val_loss)\n",
        "            print(f'Epoch {epoch + 1} - Perplexity: {perplexity:.2f}')"
      ],
      "metadata": {
        "id": "lqDBr9kJjqiu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el modelo LSTM\n",
        "def create_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length - 1))\n",
        "    model.add(LSTM(hidden_units, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    model.compile(optimizer=RMSprop(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Creamos el modelo GRU\n",
        "def create_gru_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length - 1))\n",
        "    model.add(GRU(hidden_units, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    model.compile(optimizer=RMSprop(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Creamos el modelo SimpleRNN\n",
        "def create_rnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length - 1))\n",
        "    model.add(SimpleRNN(hidden_units, dropout=0.2))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    model.compile(optimizer=RMSprop(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "QTXAXfZ6j0T6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamos el modelo**"
      ],
      "metadata": {
        "id": "RwSjgHHCvjCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegimos el modelo a entrenar (puedes cambiar entre create_lstm_model, create_gru_model o create_rnn_model)\n",
        "model = create_lstm_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7aD46DnkBrv",
        "outputId": "68b22b2c-9a14-4d83-e128-6c9f80eaaac8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el modelo\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[PerplexityCallback()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWqzIoqbkFT-",
        "outputId": "d2d144ea-d23b-4c2c-c7ae-7909e9116b50"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.0682 - loss: 10.6919Epoch 1 - Perplexity: 6490.71\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 199ms/step - accuracy: 0.0682 - loss: 10.6892 - val_accuracy: 0.0844 - val_loss: 8.7781\n",
            "Epoch 2/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.0752 - loss: 8.6474Epoch 2 - Perplexity: 3868.34\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 197ms/step - accuracy: 0.0753 - loss: 8.6468 - val_accuracy: 0.0844 - val_loss: 8.2606\n",
            "Epoch 3/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.0851 - loss: 8.1240Epoch 3 - Perplexity: 3353.31\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 197ms/step - accuracy: 0.0850 - loss: 8.1240 - val_accuracy: 0.0844 - val_loss: 8.1177\n",
            "Epoch 4/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.0791 - loss: 7.9792Epoch 4 - Perplexity: 3200.26\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 197ms/step - accuracy: 0.0791 - loss: 7.9794 - val_accuracy: 0.0844 - val_loss: 8.0710\n",
            "Epoch 5/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.0786 - loss: 7.9464Epoch 5 - Perplexity: 3230.93\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 192ms/step - accuracy: 0.0786 - loss: 7.9466 - val_accuracy: 0.0844 - val_loss: 8.0805\n",
            "Epoch 6/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.0737 - loss: 7.9916Epoch 6 - Perplexity: 3171.96\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 192ms/step - accuracy: 0.0737 - loss: 7.9915 - val_accuracy: 0.0844 - val_loss: 8.0621\n",
            "Epoch 7/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.0765 - loss: 7.9378Epoch 7 - Perplexity: 3113.21\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 216ms/step - accuracy: 0.0765 - loss: 7.9380 - val_accuracy: 0.0844 - val_loss: 8.0434\n",
            "Epoch 8/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.0838 - loss: 7.8715Epoch 8 - Perplexity: 3117.77\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 196ms/step - accuracy: 0.0838 - loss: 7.8719 - val_accuracy: 0.0844 - val_loss: 8.0449\n",
            "Epoch 9/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.0826 - loss: 7.8993Epoch 9 - Perplexity: 3113.05\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 194ms/step - accuracy: 0.0825 - loss: 7.8996 - val_accuracy: 0.0844 - val_loss: 8.0434\n",
            "Epoch 10/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - accuracy: 0.0799 - loss: 7.8753Epoch 10 - Perplexity: 3090.54\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 197ms/step - accuracy: 0.0799 - loss: 7.8756 - val_accuracy: 0.0844 - val_loss: 8.0361\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x797754d2ea70>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVACIÓN:**\n",
        "\n",
        "**Por lo que se puede observar de los resultados, el modelo no está entrenando de manera efectiva. Aunque la pérdida (loss) está disminuyendo lentamente, la precisión (accuracy) se mantiene muy baja (alrededor de 8% en todos los casos, tanto para el conjunto de entrenamiento como para el conjunto de validación). Además, la perplejidad sigue siendo extremadamente alta, lo que indica que el modelo tiene dificultades para predecir correctamente las secuencias.**"
      ],
      "metadata": {
        "id": "bKPq7A0gIo4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mejoras Aplicadas:**\n",
        "\n",
        ". Preprocesamiento: Se ha mejorado la limpieza de texto eliminando caracteres no alfanuméricos y números.\n",
        "\n",
        ". Vocabulario: El vocabulario se ha limitado a las 10,000 palabras más frecuentes (num_words=10000), lo que ayuda a reducir el espacio de búsqueda y a hacer el aprendizaje más manejable.\n",
        ". Longitud de secuencias: La longitud máxima de las secuencias se ha reducido a 50 palabras (max_sequence_length=50), lo que reduce la complejidad del modelo.\n",
        "\n",
        ". Arquitectura del modelo:\n",
        "\n",
        "Se han añadido dos capas LSTM. La primera capa tiene return_sequences=True para permitir la entrada de una segunda capa LSTM.\n",
        "Cada capa tiene dropout y recurrent dropout para ayudar a evitar el sobreajuste.\n",
        "\n",
        ". Cálculo de Perplejidad: Se ha agregado un callback personalizado para calcular la perplejidad al final de cada época.\n",
        "\n",
        ". Evaluación: Después de entrenar, se realiza una evaluación en el conjunto de validación y se imprimen las métricas de pérdida y precisión."
      ],
      "metadata": {
        "id": "us7GD5L4IxDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para preprocesar el texto\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convertimos a minúsculas\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Eliminamos caracteres no alfanuméricos\n",
        "    text = re.sub(r'\\d+', '', text)  # Eliminamos números\n",
        "    return text\n",
        "\n",
        "texts = [preprocess_text(text) for text in newsgroups.data]\n",
        "\n",
        "# Etiquetas\n",
        "labels = newsgroups.target\n",
        "\n",
        "# Configuración del Tokenizer\n",
        "vocab_size = 10000  # Reducimos el vocabulario a las 10,000 palabras más frecuentes\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convertimos textos a secuencias\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Definimos la longitud máxima de secuencias\n",
        "max_sequence_length = 50  # Reducimos la longitud de las secuencias\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Dividimos el dataset en entrenamiento y validación\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construcción del modelo LSTM mejorado\n",
        "model = Sequential()\n",
        "\n",
        "# Capa de embedding\n",
        "embedding_dim = 128\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "\n",
        "# Primera capa LSTM con return_sequences para más profundidad\n",
        "hidden_units = 128\n",
        "model.add(LSTM(hidden_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
        "\n",
        "# Segunda capa LSTM\n",
        "model.add(LSTM(hidden_units, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Capa densa de salida\n",
        "model.add(Dense(20, activation='softmax'))  # 20 clases en el dataset 20 Newsgroups\n",
        "\n",
        "# Compilación del modelo\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callback personalizado para calcular Perplejidad\n",
        "class PerplexityCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        perplexity = tf.exp(logs[\"loss\"]).numpy()  # Calcular la perplejidad\n",
        "        print(f\"Epoch {epoch + 1} - Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    callbacks=[PerplexityCallback()]\n",
        ")\n",
        "\n",
        "# Evaluación del modelo en el conjunto de validación\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD2x6nlZJS_M",
        "outputId": "b3ffe880-ed93-4f39-e2dd-4e38c82d1c0c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.1010 - loss: 2.8242Epoch 1 - Perplexity: 13.54\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 171ms/step - accuracy: 0.1012 - loss: 2.8234 - val_accuracy: 0.2669 - val_loss: 2.2357\n",
            "Epoch 2/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.3066 - loss: 2.0284Epoch 2 - Perplexity: 6.84\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 173ms/step - accuracy: 0.3068 - loss: 2.0280 - val_accuracy: 0.3805 - val_loss: 1.8772\n",
            "Epoch 3/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5052 - loss: 1.4621Epoch 3 - Perplexity: 4.23\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 164ms/step - accuracy: 0.5053 - loss: 1.4620 - val_accuracy: 0.4839 - val_loss: 1.6345\n",
            "Epoch 4/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.6321 - loss: 1.1088Epoch 4 - Perplexity: 3.03\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 166ms/step - accuracy: 0.6321 - loss: 1.1088 - val_accuracy: 0.5245 - val_loss: 1.5962\n",
            "Epoch 5/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.7268 - loss: 0.8505Epoch 5 - Perplexity: 2.36\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 169ms/step - accuracy: 0.7268 - loss: 0.8506 - val_accuracy: 0.5484 - val_loss: 1.5516\n",
            "Epoch 6/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.8009 - loss: 0.6317Epoch 6 - Perplexity: 1.93\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 163ms/step - accuracy: 0.8008 - loss: 0.6318 - val_accuracy: 0.5586 - val_loss: 1.5763\n",
            "Epoch 7/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8380 - loss: 0.5127Epoch 7 - Perplexity: 1.69\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 167ms/step - accuracy: 0.8380 - loss: 0.5128 - val_accuracy: 0.5771 - val_loss: 1.5780\n",
            "Epoch 8/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.8755 - loss: 0.4114Epoch 8 - Perplexity: 1.52\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 170ms/step - accuracy: 0.8755 - loss: 0.4114 - val_accuracy: 0.5864 - val_loss: 1.6084\n",
            "Epoch 9/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9011 - loss: 0.3273Epoch 9 - Perplexity: 1.40\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 169ms/step - accuracy: 0.9011 - loss: 0.3273 - val_accuracy: 0.5904 - val_loss: 1.7458\n",
            "Epoch 10/10\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9172 - loss: 0.2606Epoch 10 - Perplexity: 1.30\n",
            "\u001b[1m283/283\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 173ms/step - accuracy: 0.9172 - loss: 0.2606 - val_accuracy: 0.6116 - val_loss: 1.7042\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.5926 - loss: 1.8326\n",
            "Validation Loss: 1.7041512727737427, Validation Accuracy: 0.611577570438385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSIÓN**"
      ],
      "metadata": {
        "id": "l3yrpzYVPHAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**El modelo muestra una mejora significativa en precisión, alcanzando el 91.72% en entrenamiento y 61.15% en validación. Las métricas de pérdida han mostrado una tendencia a la baja, indicando que el modelo está aprendiendo de manera efectiva.\n",
        "La perplejidad también ha mejorado a lo largo de las épocas, disminuyendo de 13.54 a 1.30, lo que sugiere una mejor comprensión del modelo sobre los datos.\n",
        "Sin embargo, se observan leves signos de sobreajuste en las últimas épocas, ya que la pérdida de validación no disminuye de manera similar.\n",
        "En general, el modelo presenta un buen potencial para clasificar correctamente las categorias del dataset.**"
      ],
      "metadata": {
        "id": "8GPPTzeQPNqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generación de Texto - Greedy Search**"
      ],
      "metadata": {
        "id": "4Sngf8Qjv4Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_sample(model, tokenizer, seed_text, num_words):\n",
        "    for _ in range(num_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted_word_index = np.random.choice(len(predicted_probs), p=predicted_probs/np.sum(predicted_probs))  # Muestreo\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
        "        if not predicted_word:\n",
        "            break\n",
        "        seed_text += ' ' + predicted_word\n",
        "    return seed_text\n",
        "\n",
        "seed_text = \"the impact of artificial intelligence on society\"\n",
        "generated_text = generate_text_sample(model, tokenizer, seed_text, num_words=50)\n",
        "print(f'Generated text (Sample): {generated_text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igPFWUuLV5JE",
        "outputId": "07b93588-79b0-4efd-d197-6a8976313187"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text (Sample): the impact of artificial intelligence on society on you is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generación de texto - Beam Search**"
      ],
      "metadata": {
        "id": "nBFzwv47xf0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-10) / temperature  # Evitar log(0)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return np.random.choice(len(preds), p=preds)\n",
        "\n",
        "def beam_search_predict(model, tokenizer, seed_text, beam_width=3, max_sequence_length=50, temperature=1.0):\n",
        "    sequences = [[seed_text, 0.0]]  # Secuencias iniciales y sus puntajes\n",
        "    for _ in range(max_sequence_length):\n",
        "        all_candidates = []\n",
        "        for seq, score in sequences:\n",
        "            token_list = tokenizer.texts_to_sequences([seq])[0]\n",
        "            token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
        "            predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "            top_k = np.argsort(predicted_probs)[-beam_width:]  # Obtener las top_k palabras\n",
        "\n",
        "            for word_index in top_k:\n",
        "                # Usar el método de muestreo\n",
        "                sampled_word_index = sample(predicted_probs, temperature)\n",
        "                predicted_word = tokenizer.index_word.get(sampled_word_index, '')\n",
        "\n",
        "                if predicted_word:  # Asegúrate de que no sea una palabra vacía\n",
        "                    candidate = seq + ' ' + predicted_word\n",
        "                    candidate_score = score - np.log(predicted_probs[sampled_word_index] + 1e-10)  # Agregar el puntaje\n",
        "                    all_candidates.append([candidate, candidate_score])\n",
        "\n",
        "        sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_width]  # Mantener las mejores\n",
        "\n",
        "    return sequences[0][0]\n",
        "\n",
        "# Generamos texto usando la función ajustada\n",
        "generated_text_beam = beam_search_predict(model, tokenizer, seed_text, beam_width=10, temperature=0.5)\n",
        "print(f'Generated text (Beam Search): {generated_text_beam}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDG9PKJGXWhh",
        "outputId": "5a379681-0a07-4b59-bfb4-e4984328d6ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text (Beam Search): the impact of artificial intelligence on society on on on on on on on on this this this this this this you you you you you you you you you you you it it it it it it it i i i i to to to to to to to to to ax ax to to to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generación de texto con Temperatura - Busqueda Estocástica**"
      ],
      "metadata": {
        "id": "RauzZTvkwG1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_search_predict(model, tokenizer, seed_text, num_words, temperature=1.0, max_sequence_length=50):\n",
        "    generated_text = seed_text\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
        "\n",
        "        # Predicciones de probabilidad\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "        # Aplicamos la temperatura\n",
        "        exp_probs = np.exp(predicted_probs / temperature)\n",
        "        prob_distribution = exp_probs / np.sum(exp_probs)  # Normalización\n",
        "\n",
        "        # Muestreo de la distribución de probabilidad\n",
        "        predicted_word_index = np.random.choice(range(len(prob_distribution)), p=prob_distribution)\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
        "\n",
        "        if not predicted_word:\n",
        "            break\n",
        "\n",
        "        generated_text += ' ' + predicted_word\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Parámetros\n",
        "seed_text = \"the impact of artificial intelligence on society\"\n",
        "generated_text_stochastic = stochastic_search_predict(model, tokenizer, seed_text, num_words=50, temperature=0.7)\n",
        "print(f'Generated text (Stochastic Search): {generated_text_stochastic}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23B6MTRrY3Zx",
        "outputId": "b86ff7be-2974-404c-a071-7308fb4a3989"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text (Stochastic Search): the impact of artificial intelligence on society edu it ax on to ax from is t edu is on on you and this in is ax it you you you is from ax of this a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSIÓN**\n",
        "\n",
        "**En la generación de texto, se probaron tres estrategias: Greedy Search, Beam Search y Búsqueda Estocástica. La Greedy Search generó resultados limitados. la Beam Search mostró una ligera mejora al considerar múltiples secuencias, aunque aún presentaba ciertas redundancias. Finalmente, la Búsqueda Estocástica ofreció los resultados más variados y coherentes, al introducir aleatoriedad mediante el muestreo de palabras, lo que permitió una generación de texto más rica y creativa. En conclusión, la Búsqueda Estocástica fue la más efectiva para lograr diversidad y fluidez en el texto generado.**"
      ],
      "metadata": {
        "id": "GrKHYrNAbR9I"
      }
    }
  ]
}